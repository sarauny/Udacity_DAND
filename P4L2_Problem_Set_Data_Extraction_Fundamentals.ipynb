{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "import pprint\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quiz: Using CSV Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYour task is to process the supplied file and use the csv module to extract data from it.\\nThe data comes from NREL (National Renewable Energy Laboratory) website. Each file\\ncontains information from one meteorological station, in particular - about amount of\\nsolar and wind energy for each hour of day.\\n\\nNote that the first line of the datafile is neither data entry, nor header. It is a line\\ndescribing the data source. You should extract the name of the station from it.\\n\\nThe data should be returned as a list of lists (not dictionaries).\\nYou can use the csv modules \"reader\" method to get data in such format.\\nAnother useful method is next() - to get the next line from the iterator.\\nYou should only change the parse_file function.\\n'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################\n",
    "#                 1                 #\n",
    "#####################################\n",
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATADIR = \"P4L2_Resources/\"\n",
    "DATAFILE = \"745090.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        name = reader.next()[1]\n",
    "        reader.next()\n",
    "        for row in reader:\n",
    "                \n",
    "            data.append(row)\n",
    "            \n",
    "    # Do not change the line below\n",
    "    return (name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test if the code above is correct\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quiz: Excel To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFind the time and value of max load for each of the regions\\nCOAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\\nand write the result out in a csv file, using pipe character | as the delimiter.\\n\\nAn example output can be seen in the \"example.csv\" file.\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################\n",
    "#                 2                 #\n",
    "#####################################\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expected output\n",
    "# Station|Year|Month|Day|Hour|Max Load\n",
    "# COAST|2013|01|01|10|12345.6\n",
    "# EAST|2013|01|01|10|12345.6\n",
    "# FAR_WEST|2013|01|01|10|12345.6\n",
    "# NORTH|2013|01|01|10|12345.6\n",
    "# NORTH_C|2013|01|01|10|12345.6\n",
    "# SOUTHERN|2013|01|01|10|12345.6\n",
    "# SOUTH_C|2013|01|01|10|12345.6\n",
    "# WEST|2013|01|01|10|12345.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = \"P4L1_Resources/2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"P4L2_Resources/2013_Max_Loads.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract csv file from zip file\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = None\n",
    "# YOUR CODE HERE\n",
    "# Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "# Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "    data_for_newFrame = []\n",
    "    # read data on sheet\n",
    "    data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    \n",
    "    # read each column in data and look for max value\n",
    "    # and its date and load amount\n",
    "    for q in range(1,9):\n",
    "        cv = sheet.col_values(q, start_rowx=1, end_rowx=None)\n",
    "        station = sheet.cell_value(0,q)\n",
    "        maxval = max(cv)\n",
    "        \n",
    "        maxpos = cv.index(maxval) + 1\n",
    "        maxtime = sheet.cell_value(maxpos, 0)\n",
    "\n",
    "        realtime = xlrd.xldate_as_tuple(maxtime, 0)\n",
    "        print(realtime)\n",
    "\n",
    "        data_for_newFrame.append([str(station), realtime, maxval])\n",
    "#         data[station] = {\n",
    "#                        \"maxval\": maxval,\n",
    "# #                        \"maxtime\": realtime,\n",
    "#                          }\n",
    "#     #print data\n",
    "    return data_for_newFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2013, 8, 13, 17, 0, 0)\n",
      "(2013, 8, 5, 17, 0, 0)\n",
      "(2013, 6, 26, 17, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "(2013, 8, 7, 18, 0, 0)\n",
      "(2013, 8, 8, 16, 0, 0)\n",
      "(2013, 8, 8, 18, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['COAST', (2013, 8, 13, 17, 0, 0), 18779.025510000003],\n",
       " ['EAST', (2013, 8, 5, 17, 0, 0), 2380.1654089999956],\n",
       " ['FAR_WEST', (2013, 6, 26, 17, 0, 0), 2281.2722140000024],\n",
       " ['NORTH', (2013, 8, 7, 17, 0, 0), 1544.7707140000005],\n",
       " ['NORTH_C', (2013, 8, 7, 18, 0, 0), 24415.570226999993],\n",
       " ['SOUTHERN', (2013, 8, 8, 16, 0, 0), 5494.157645],\n",
       " ['SOUTH_C', (2013, 8, 8, 18, 0, 0), 11433.30491600001],\n",
       " ['WEST', (2013, 8, 7, 17, 0, 0), 1862.6137649999998]]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_file(data, filename):\n",
    "    # YOUR CODE HERE\n",
    "    with open(filename, \"w\") as f:\n",
    "        w = csv.writer(f, delimiter=\"|\")\n",
    "        w.writerow([\"Station\",\"Year\",\"Month\",\"Day\",\"Hour\",\"Max Load\"])\n",
    "        for s in data:\n",
    "            year, month, day, hour, _ ,_ = s[1]\n",
    "            print(s[0], year, month, day, hour, s[2])\n",
    "            w.writerow([s[0], year, month, day, hour, s[2]])     \n",
    "# #             year, month, day, hour, _ ,_ = data[s][\"maxtime\"]\n",
    "#             w.writerow([s, year, month, day, hour, data[s][\"maxval\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2013, 8, 13, 17, 0, 0)\n",
      "(2013, 8, 5, 17, 0, 0)\n",
      "(2013, 6, 26, 17, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "(2013, 8, 7, 18, 0, 0)\n",
      "(2013, 8, 8, 16, 0, 0)\n",
      "(2013, 8, 8, 18, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "('COAST', 2013, 8, 13, 17, 18779.025510000003)\n",
      "('EAST', 2013, 8, 5, 17, 2380.1654089999956)\n",
      "('FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024)\n",
      "('NORTH', 2013, 8, 7, 17, 1544.7707140000005)\n",
      "('NORTH_C', 2013, 8, 7, 18, 24415.570226999993)\n",
      "('SOUTHERN', 2013, 8, 8, 16, 5494.157645)\n",
      "('SOUTH_C', 2013, 8, 8, 18, 11433.30491600001)\n",
      "('WEST', 2013, 8, 7, 17, 1862.6137649999998)\n"
     ]
    }
   ],
   "source": [
    "save_file(parse_file(datafile), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2013, 8, 13, 17, 0, 0)\n",
      "(2013, 8, 5, 17, 0, 0)\n",
      "(2013, 6, 26, 17, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "(2013, 8, 7, 18, 0, 0)\n",
      "(2013, 8, 8, 16, 0, 0)\n",
      "(2013, 8, 8, 18, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "('COAST', 2013, 8, 13, 17, 18779.025510000003)\n",
      "('EAST', 2013, 8, 5, 17, 2380.1654089999956)\n",
      "('FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024)\n",
      "('NORTH', 2013, 8, 7, 17, 1544.7707140000005)\n",
      "('NORTH_C', 2013, 8, 7, 18, 24415.570226999993)\n",
      "('SOUTHERN', 2013, 8, 8, 16, 5494.157645)\n",
      "('SOUTH_C', 2013, 8, 8, 18, 11433.30491600001)\n",
      "('WEST', 2013, 8, 7, 17, 1862.6137649999998)\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'COAST', 'Year': '2013', 'Day': '13', 'Max Load': '18779.025510000003'}\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'EAST', 'Year': '2013', 'Day': '5', 'Max Load': '2380.1654089999956'}\n",
      "{'Hour': '17', 'Month': '6', 'Station': 'FAR_WEST', 'Year': '2013', 'Day': '26', 'Max Load': '2281.2722140000024'}\n",
      "2013\n",
      "2013\n",
      "6\n",
      "6\n",
      "26\n",
      "26\n",
      "17\n",
      "17\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'NORTH', 'Year': '2013', 'Day': '7', 'Max Load': '1544.7707140000005'}\n",
      "{'Hour': '18', 'Month': '8', 'Station': 'NORTH_C', 'Year': '2013', 'Day': '7', 'Max Load': '24415.570226999993'}\n",
      "{'Hour': '16', 'Month': '8', 'Station': 'SOUTHERN', 'Year': '2013', 'Day': '8', 'Max Load': '5494.157645'}\n",
      "{'Hour': '18', 'Month': '8', 'Station': 'SOUTH_C', 'Year': '2013', 'Day': '8', 'Max Load': '11433.30491600001'}\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'WEST', 'Year': '2013', 'Day': '7', 'Max Load': '1862.6137649999998'}\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            print(line)\n",
    "            station = line['Station']\n",
    "            \n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    #print(field)\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "    \n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        print(ans[station][field])\n",
    "                        print(line[field])\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\\\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2013, 8, 13, 17, 0, 0)\n",
      "(2013, 8, 5, 17, 0, 0)\n",
      "(2013, 6, 26, 17, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "(2013, 8, 7, 18, 0, 0)\n",
      "(2013, 8, 8, 16, 0, 0)\n",
      "(2013, 8, 8, 18, 0, 0)\n",
      "(2013, 8, 7, 17, 0, 0)\n",
      "('COAST', 2013, 8, 13, 17, 18779.025510000003)\n",
      "('EAST', 2013, 8, 5, 17, 2380.1654089999956)\n",
      "('FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024)\n",
      "('NORTH', 2013, 8, 7, 17, 1544.7707140000005)\n",
      "('NORTH_C', 2013, 8, 7, 18, 24415.570226999993)\n",
      "('SOUTHERN', 2013, 8, 8, 16, 5494.157645)\n",
      "('SOUTH_C', 2013, 8, 8, 18, 11433.30491600001)\n",
      "('WEST', 2013, 8, 7, 17, 1862.6137649999998)\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'COAST', 'Year': '2013', 'Day': '13', 'Max Load': '18779.025510000003'}\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'EAST', 'Year': '2013', 'Day': '5', 'Max Load': '2380.1654089999956'}\n",
      "{'Hour': '17', 'Month': '6', 'Station': 'FAR_WEST', 'Year': '2013', 'Day': '26', 'Max Load': '2281.2722140000024'}\n",
      "2013\n",
      "2013\n",
      "6\n",
      "6\n",
      "26\n",
      "26\n",
      "17\n",
      "17\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'NORTH', 'Year': '2013', 'Day': '7', 'Max Load': '1544.7707140000005'}\n",
      "{'Hour': '18', 'Month': '8', 'Station': 'NORTH_C', 'Year': '2013', 'Day': '7', 'Max Load': '24415.570226999993'}\n",
      "{'Hour': '16', 'Month': '8', 'Station': 'SOUTHERN', 'Year': '2013', 'Day': '8', 'Max Load': '5494.157645'}\n",
      "{'Hour': '18', 'Month': '8', 'Station': 'SOUTH_C', 'Year': '2013', 'Day': '8', 'Max Load': '11433.30491600001'}\n",
      "{'Hour': '17', 'Month': '8', 'Station': 'WEST', 'Year': '2013', 'Day': '7', 'Max Load': '1862.6137649999998'}\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quiz: Wrangling JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis exercise shows some important concepts that you should be aware about:\\n- using codecs module to write unicode files\\n- using authentication with web APIs\\n- using offset when accessing web APIs\\n\\nTo run this code locally you have to register at the NYTimes developer site \\nand get your own API key. You will be able to complete this exercise in our UI\\nwithout doing so, as we have provided a sample result. (See the file \\n\\'popular-viewed-1.json\\' from the tabs above.)\\n\\n\\n\"Here\\'s your API Key for the Article Search API: beb94741d85547e5af1f0c10c72e58ec\"\\n\\n\\nYour task is to modify the article_overview() function to process the saved\\nfile that represents the most popular articles (by view count) from the last\\nday, and return a tuple of variables containing the following data:\\n- labels: list of dictionaries, where the keys are the \"section\" values and\\n  values are the \"title\" values for each of the retrieved articles.\\n- urls: list of URLs for all \\'media\\' entries with \"format\": \"Standard Thumbnail\"\\n\\nAll your changes should be in the article_overview() function. See the test() \\nfunction for examples of the elements of the output lists.\\nThe rest of functions are provided for your convenience, if you want to access\\nthe API by yourself.\\n'"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################\n",
    "#                 3                 #\n",
    "#####################################\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI\n",
    "without doing so, as we have provided a sample result. (See the file \n",
    "'popular-viewed-1.json' from the tabs above.)\n",
    "\n",
    "\n",
    "\"Here's your API Key for the Article Search API: beb94741d85547e5af1f0c10c72e58ec\"\n",
    "\n",
    "\n",
    "Your task is to modify the article_overview() function to process the saved\n",
    "file that represents the most popular articles (by view count) from the last\n",
    "day, and return a tuple of variables containing the following data:\n",
    "- labels: list of dictionaries, where the keys are the \"section\" values and\n",
    "  values are the \"title\" values for each of the retrieved articles.\n",
    "- urls: list of URLs for all 'media' entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview() function. See the test() \n",
    "function for examples of the elements of the output lists.\n",
    "The rest of functions are provided for your convenience, if you want to access\n",
    "the API by yourself.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"beb94741d85547e5af1f0c10c72e58ec\",\n",
    "            \"article\": \"beb94741d85547e5af1f0c10c72e58ec\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for article in data:\n",
    "        section = article[\"section\"]\n",
    "        title = article[\"title\"]\n",
    "        titles.append({section: title})\n",
    "        if \"media\" in article:\n",
    "            for m in article[\"media\"]:\n",
    "                for mm in m[\"media-metadata\"]:\n",
    "                    if mm[\"format\"] == \"Standard Thumbnail\":\n",
    "                        urls.append(mm[\"url\"])\n",
    "\n",
    "    return (titles, urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-492-dc86dbc2526a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-492-dc86dbc2526a>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_overview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"viewed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Opinion'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Professors, We Need You!'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test if the code above is correct\n",
    "\n",
    "def test():\n",
    "    \n",
    "    save_file(kind, period)\n",
    "    \n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    \n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
