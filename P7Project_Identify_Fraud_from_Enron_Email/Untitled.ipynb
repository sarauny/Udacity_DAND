{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns: 21\n",
      "name of the columns:\n",
      "Index([u'bonus', u'deferral_payments', u'deferred_income', u'director_fees',\n",
      "       u'email_address', u'exercised_stock_options', u'expenses',\n",
      "       u'from_messages', u'from_poi_to_this_person',\n",
      "       u'from_this_person_to_poi', u'loan_advances', u'long_term_incentive',\n",
      "       u'other', u'poi', u'restricted_stock', u'restricted_stock_deferred',\n",
      "       u'salary', u'shared_receipt_with_poi', u'to_messages',\n",
      "       u'total_payments', u'total_stock_value'],\n",
      "      dtype='object')\n",
      "number of non pois / pois: 128 / 18\n",
      "Number of missing values in data\n",
      "poi                            0\n",
      "total_stock_value             20\n",
      "total_payments                21\n",
      "email_address                 35\n",
      "restricted_stock              36\n",
      "exercised_stock_options       44\n",
      "salary                        51\n",
      "expenses                      51\n",
      "other                         53\n",
      "to_messages                   60\n",
      "shared_receipt_with_poi       60\n",
      "from_messages                 60\n",
      "from_poi_to_this_person       60\n",
      "from_this_person_to_poi       60\n",
      "bonus                         64\n",
      "long_term_incentive           80\n",
      "deferred_income               97\n",
      "deferral_payments            107\n",
      "restricted_stock_deferred    128\n",
      "director_fees                129\n",
      "loan_advances                142\n",
      "Name: nan, dtype: int64\n",
      "Ratio of missing values in POI / non-POIs\n",
      "                           poi_nan_ratio  non_poi_nan_ratio\n",
      "total_stock_value                      0                 15\n",
      "poi                                    0                  0\n",
      "email_address                          0                 27\n",
      "other                                  0                 41\n",
      "expenses                               0                 39\n",
      "total_payments                         0                 16\n",
      "salary                                 5                 39\n",
      "restricted_stock                       5                 27\n",
      "bonus                                 11                 48\n",
      "from_this_person_to_poi               22                 43\n",
      "from_messages                         22                 43\n",
      "shared_receipt_with_poi               22                 43\n",
      "to_messages                           22                 43\n",
      "from_poi_to_this_person               22                 43\n",
      "long_term_incentive                   33                 57\n",
      "exercised_stock_options               33                 29\n",
      "deferred_income                       38                 70\n",
      "deferral_payments                     72                 73\n",
      "loan_advances                         94                 97\n",
      "director_fees                        100                 86\n",
      "restricted_stock_deferred            100                 85\n",
      "max: TOTAL 97343619.0\n",
      "people whose both bonus and salary are away from other data points\n",
      "['LAY KENNETH L', 'SKILLING JEFFREY K']\n",
      "people whose only bonus is away from other data points\n",
      "['LAVORATO JOHN J', 'BELDEN TIMOTHY N']\n",
      "people whose only bonus is away from other data points\n",
      "['WHALLEY LAWRENCE G', 'PICKERING MARK R', 'FREVERT MARK A']\n",
      "       LAY KENNETH L: True\n",
      "  SKILLING JEFFREY K: True\n",
      "     LAVORATO JOHN J: False\n",
      "    BELDEN TIMOTHY N: True\n",
      "  WHALLEY LAWRENCE G: False\n",
      "    PICKERING MARK R: False\n",
      "      FREVERT MARK A: False\n",
      "number of non-pois / pois: 123 / 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [8] are constant.\n",
      "  UserWarning)\n",
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, precision and recall scores of each classifier\n",
      "                                                 clf     score  precision    recall\n",
      "0                                       GaussianNB()  0.357143   0.420168  0.357143\n",
      "1  SVC(C=1.0, cache_size=200, class_weight=None, ...  0.476190   0.226757  0.476190\n",
      "2  DecisionTreeClassifier(class_weight=None, crit...  0.452381   0.455890  0.452381\n",
      "feature importances by order\n",
      "deferral_payments : 0.326375362507\n",
      "deferred_income : 0.0816206474563\n",
      "director_fees : 0.0772527867539\n",
      "exercised_stock_options : 0.0696879193178\n",
      "expenses : 0.0645413613473\n",
      "from_messages : 0.060629972979\n",
      "from_poi_to_this_person : 0.0433510133549\n",
      "from_this_person_to_poi : 0.0425250461131\n",
      "loan_advances : 0.04090377873\n",
      "long_term_incentive : 0.0393888239623\n",
      "other : 0.0360736825123\n",
      "poi : 0.029996104402\n",
      "restricted_stock : 0.0254512400987\n",
      "restricted_stock_deferred : 0.0240254003172\n",
      "salary : 0.0199974029347\n",
      "shared_receipt_with_poi : 0.00908972860667\n",
      "to_messages : 0.00908972860667\n",
      "total_payments : 0.0\n",
      "total_stock_value : 0.0\n",
      "accuracy, precision and recall scores of each classifier\n",
      "                                                 clf     score  precision    recall\n",
      "0                                       GaussianNB()  0.357143   0.420168  0.357143\n",
      "1  SVC(C=1.0, cache_size=200, class_weight=None, ...  0.476190   0.226757  0.476190\n",
      "2  DecisionTreeClassifier(class_weight=None, crit...  0.452381   0.463033  0.452381\n",
      "grid search score\n",
      "           parameters  mean_validation_score                               cv_validation_scores\n",
      "0    {u'kbest__k': 1}               0.438776  [0.138888888889, 0.277777777778, 0.45454545454...\n",
      "1    {u'kbest__k': 2}               0.377551  [0.138888888889, 0.277777777778, 0.18181818181...\n",
      "2    {u'kbest__k': 3}               0.367347  [0.111111111111, 0.222222222222, 0.27272727272...\n",
      "3    {u'kbest__k': 4}               0.397959  [0.166666666667, 0.222222222222, 0.36363636363...\n",
      "4    {u'kbest__k': 5}               0.367347  [0.111111111111, 0.166666666667, 0.27272727272...\n",
      "5    {u'kbest__k': 6}               0.367347  [0.138888888889, 0.111111111111, 0.27272727272...\n",
      "6    {u'kbest__k': 7}               0.408163  [0.138888888889, 0.166666666667, 0.45454545454...\n",
      "7    {u'kbest__k': 8}               0.428571  [0.138888888889, 0.166666666667, 0.63636363636...\n",
      "8    {u'kbest__k': 9}               0.428571  [0.111111111111, 0.222222222222, 0.54545454545...\n",
      "9   {u'kbest__k': 10}               0.438776  [0.111111111111, 0.166666666667, 0.72727272727...\n",
      "10  {u'kbest__k': 11}               0.418367  [0.111111111111, 0.166666666667, 0.45454545454...\n",
      "11  {u'kbest__k': 12}               0.418367  [0.111111111111, 0.222222222222, 0.45454545454...\n",
      "12  {u'kbest__k': 13}               0.418367  [0.111111111111, 0.222222222222, 0.45454545454...\n",
      "13  {u'kbest__k': 14}               0.418367  [0.138888888889, 0.166666666667, 0.45454545454...\n",
      "14  {u'kbest__k': 15}               0.397959  [0.111111111111, 0.166666666667, 0.45454545454...\n",
      "15  {u'kbest__k': 16}               0.418367  [0.111111111111, 0.222222222222, 0.45454545454...\n",
      "16  {u'kbest__k': 17}               0.418367  [0.194444444444, 0.111111111111, 0.45454545454...\n",
      "17  {u'kbest__k': 18}               0.387755  [0.111111111111, 0.111111111111, 0.45454545454...\n",
      "18  {u'kbest__k': 19}               0.418367  [0.166666666667, 0.166666666667, 0.45454545454...\n",
      "19  {u'kbest__k': 20}               0.408163  [0.166666666667, 0.166666666667, 0.36363636363...\n",
      "20  {u'kbest__k': 21}               0.438776  [0.194444444444, 0.166666666667, 0.45454545454...\n",
      "\n",
      "best parameters set\n",
      "{'kbest__k': 1}\n",
      "\n",
      "evaluation of classifier when k is best_k\n",
      "score: 0.47619047619 precision: 0.226757369615 recall: 0.47619047619\n",
      "restricted_stock_deferred: inf\n",
      "          deferred_income: 79.9827900546\n",
      "                 expenses: 23.3918119366\n",
      "        fraction_from_poi: 12.9934772788\n",
      "          fraction_to_poi: 11.2379835046\n",
      "                    bonus: 10.5672584851\n",
      "            loan_advances: 8.12899988806\n",
      "  exercised_stock_options: 5.61261591628\n",
      "                    other: 5.28932061756\n",
      "            from_messages: 4.48498424278\n",
      "            director_fees: 3.35957416012\n",
      "  from_poi_to_this_person: 3.30388037268\n",
      "        total_stock_value: 3.00662658583\n",
      "              to_messages: 2.09915411291\n",
      "  shared_receipt_with_poi: 1.98767174959\n",
      "         restricted_stock: 1.85308683796\n",
      "        deferral_payments: 1.68084067374\n",
      "      long_term_incentive: 1.27694171373\n",
      "                   salary: 1.00314536711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [6] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters set\n",
      "{'DT__criterion': 'gini', 'DT__max_depth': None, 'DT__min_samples_leaf': 2, 'DT__min_samples_split': 6}\n",
      "0.785714285714 0.2 0.166666666667\n",
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('kbest', SelectKBest(k=17, score_func=<function f_classif at 0x110a01050>)), ('DT', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))]),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'DT__criterion': ['gini', 'entropy'], 'DT__max_depth': [None, 5, 10, 15, 20], 'DT__min_samples_split': [2, 4, 6, 8, 10, 20], 'DT__min_samples_leaf': [2, 4, 6, 8, 10, 20]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "/Users/sarauenoyama/anaconda2/envs/DAND/lib/python2.7/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAADmCAYAAAD2m94SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmYXVWZ7/HvLyGAiVIMkQTbKENAofUGqpgikoAJhMEO\nILRYgYCI9gWiaNlO/dg0F/oqqAyikgZFCTFS16C2RKYKiQwthMEqAg6EhBAEFAJJoAKEIEm994+1\ni5wUdWo4OafOUL/P85yn6qy91j7vXtmpemuttfdWRGBmZmZWyYaUOwAzMzOz3jhhMTMzs4rnhMXM\nzMwqnhMWMzMzq3hOWMzMzKziOWExMzOziueExczMzCqeExYzMzOreE5YzMzMrOI5YTEzM7OK54Sl\nF5IOlTRP0l8ldUiaWsA+pkhaJGmtpOcl/ULSe0sRr5mZWS1ywtK7EcBi4Byg3w9ekrQr8GtgATAO\nOBIYCfyyaBGamZnVOPnhh30nqQM4PiLm5ZRtDXwT+ASwPfAH4GsRcVe2/UTg+ojYJqfNR0lJzDYR\nsXEAD8HMzKwqeYRly10JHAR8HPggcANwq6Q9su2tQIekMyQNkVQHTAdud7JiZmbWNx5h6YeuIyyS\nxgBPAGMi4rmcercD90fEv2fvJwBzgZ2AocC9wDERsXaAD8HMzKwqeYRly3yQlIAslfRy5wuYAOwB\nIGkU8CPgWmD/bNsbeA2LmZlZn21V7gCq3NuBDUA90NFl2yvZ1xlAe0T8W+cGSacCT0s6MCIeGJBI\nzczMqpgTli3zEGmEZVRE3JOnznBSUpOrM7nxCJeZmVkfVN0vzP7eF0XSCZLmZ/c/aZd0r6Qj+/F5\nIySNk7RvVrR79n5MRCwDrgdmZ5+zq6QDJX1N0tFZ/ZuBAyWdJ2mspHrS9NAKUsJjZmZmvai6hIX+\n3xdlAjAfOJo0dXMH8BtJ4/r4efuTEovW7PMuBdqAC7LtnwRmA5cAS4BfZW2eAoiIO4BpwHFZu1uA\n14CjI+L1PsZgZmY2qFX1VULd3Relj+3+CPy/iPi/pYnMzMzMiqkaR1i2iCQB7wDWlDsWMzMz65tB\nl7AAXyZNK80tdyBmZmbWN4PqKiFJ04DzgKkRsaqHejsBU4AngfUDE52ZmVlN2BbYFWiJiNXF2umg\nSVgkfQL4IXBSthC2J1OAn5U+KjMzs5p1CulK2qIYFAmLpEbgGuDkiLitD02eBJgzZw577713KUOr\nCk1NTVx++eXlDqPs3A+buC8S90PiftjEfQGPPvoop556KmS/S4ul6hIWSSOAsYCyot2zS5TXRMTT\nki4C3hURp2f1pwGzgHOBB7Nb5QO81sOzfNYD7L333tTX15foSKpHXV2d+wH3Qy73ReJ+SNwPm7gv\nNlPUJRXVuOi2t/uijAbG5NT/DOlutFcCf8t5fXeA4jUzM7MtVHUjLBFxFz0kWhFxRpf3h5c8KDMz\nMyupahxhMTMzs0HGCYv1qrGxsdwhVAT3wybui8T9kLgfNnFflE5V35q/VLIHFLa2trZ68ZSZmVk/\ntLW10dDQANAQEW3F2q9HWMzMzKziOWExMzOziueExczMzCqeExYzMzOreE5YzMzMrOI5YTEzM7OK\n54TFzMzMKp4TFjMzM6t4VZewSDpU0jxJf5XUIWlqH9ocJqlV0npJSyWdPhCxmpmZWXFUXcICjAAW\nA+eQntbcI0m7AjcBC4FxwBXANZKOKF2IZmZmVkzV+LTm24DbACSpD03OBp6IiK9k7x+T9GGgCbi9\nNFGamZlZMVXjCEt/HQws6FLWAowvQyxmZmZWgMGQsIwGVnYpWwlsJ2mbMsRjZmZm/VR1U0JmZmaD\nydKlS1m+fDljx45lzz33LHc4ZTMYEpbngFFdykYBayPi9Z4aNjU1UVdXt1lZY2MjjY2NxY3QzMys\nizVr1jBt2nRaWm55s2zKlGNobp7DDjvsUMbINmlubqa5uXmzsvb29pJ8liJ6vdCmYknqAI6PiHk9\n1LkYODoixuWUXQ9sHxHH5GlTD7S2trZSX19f7LDNzMx6ddRRx7JgwX1s3Pg9YAJwN0OHnsvkyQdz\n2203lzu8vNra2mhoaABoiIi2Yu236kZYJI0AxgKdVwjtLmkcsCYinpZ0EfCuiOi818pVwAxJ3wJ+\nAkwCTgK6TVbMzMzKbenSpdnIyhzglKz0FDZuDFpaprNs2bJBNz1UjYtu9wceAlpJ92G5FGgDLsi2\njwbGdFaOiCeBY4HJpPu3NAFnRkTXK4fMzMwqwvLly7PvJnTZMhGAxx9/fEDjqQRVN8ISEXfRQ6IV\nEWd0U3Y30FDKuMzMzIpljz32yL67m00jLAB3ATB27NiBDqnsqnGExczMrKbttddeTJlyDEOHnkua\nFnoamMPQoZ9nypRjBt10EDhhMTMzq0jNzXOYPPlgYDrwHmA6kycfTHPznDJHVh5VNyVkZmY2GOyw\nww7cdtvNLFu2jMcff9z3YSl3AGZmZpbfnnvuOagTlU6eEjIzM7OK54TFzMzMKp4TFjMzM6t4TljM\nzMys4jlhMTMzs4rnhMXMzMwqnhMWMzMzq3hVmbBImiFphaTXJN0n6YBe6p8iabGkVyX9TdKPJe04\nUPGamZnZlqm6hEXSyaQnNJ8P7Ac8DLRIGpmn/iHAdcCPgH2Ak4ADgR8OSMBmZma2xaouYQGagKsj\nYnZELAHOAtYBn8pT/2BgRURcGRF/iYh7gatJSYuZmZlVgapKWCQNAxqAhZ1lERHAAmB8nmaLgDGS\njs72MQr4Z+Dm0kZrZmZmxVJVCQswEhgKrOxSvhIY3V2DbETlVODnkv4OPAu8CHy2hHGamZlZEVVb\nwtJvkvYBrgD+D1APTAF2I00LmZmZWRWotqc1rwI2AqO6lI8CnsvT5mvAPRFxWfb+j5LOAf5H0tcj\noutozZuampqoq6vbrKyxsZHGxsaCgjczM6slzc3NNDc3b1bW3t5eks9SWgJSPSTdB9wfEZ/P3gt4\nCvheRHynm/q/AP4eEdNyysYDvwP+ISLekuhIqgdaW1tbqa+vL9GRmJmZ1Z62tjYaGhoAGiKirVj7\nrcYpocuAz0g6TdL7gauA4cAsAEkXSboup/5vgBMlnSVpt+wy5ytISU++URkzMzOrINU2JUREzM3u\nuXIhaSpoMTAlIl7IqowGxuTUv07S24EZwCXAS6SrjL42oIGbmZlZwaouYQGIiJnAzDzbzuim7Erg\nylLHZWZmZqVRjVNCZmZmNsg4YTEzM7OK54TFzMzMKp4TFjMzM6t4TljMzMys4jlhMTMzs4rnhMXM\nzMwqnhMWMzMzq3hOWMzMzKziOWExMzOziueExczMzCpeURIWSdtJOl7S3sXYn5mZmVmughIWSXMl\nfTb7/m3A74G5wCOSTixifPk+f4akFZJek3SfpAN6qb+1pG9IelLSeklPSPpkqeM0MzOz4ih0hGUC\n8D/Z9ycAArYHzgX+vQhx5SXpZOBS4HxgP+BhoEXSyB6a3QAcDpwB7AU0Ao+VMk4zMzMrnkITljpg\nTfb9UcAvI2IdcDOwZzEC60ETcHVEzI6IJcBZwDrgU91VlnQUcChwTETcERFPRcT9EbGoxHGamZlZ\nkRSasDwNjJc0gpSwzM/KdwDWFyOw7kgaBjQACzvLIiKABcD4PM3+iTRl9VVJz0h6TNJ3JG1bqjjN\nzMysuLYqsN13gZ8BrwB/Ae7MyicAf9jysPIaCQwFVnYpXwm8L0+b3UkjLOuB47N9/BewI3BmacI0\nMzOzYiooYYmImZIeAMYAt0dER7bpCUq8hqUAQ4AOYFpEvAIg6YvADZLOiYjX8zVsamqirq5us7LG\nxkYaGxtLGa+ZmVlVaG5uprm5ebOy9vb2knyW0oxKdcimhNYBJ0bEvJzyWUBdRJzQTZtZwIciYq+c\nsvcDfwL2iojl3bSpB1pbW1upr68v+nGYmZnVqra2NhoaGgAaIqKtWPstaIRF0k962h4R3S6A3VIR\n8YakVmASMC+LRdn77+Vpdg9wkqTh2cJgSNNHHcAzpYjTzMzMiqvQRbc7dHntDHwE+Bjp8uZSugz4\njKTTspGSq4DhwCwASRdJui6n/vXAauBaSXtLmgB8G/hxT9NBZmZmVjkKXcPS3dTLENJi1rdMsRRT\nRMzN7rlyITAKWAxMiYgXsiqjSWtrOuu/KukI4PvAg6Tk5efAeaWM08zMzIqn0KuE3iIiOiRdRrpi\n6NvF2m+ez5oJzMyz7YxuypYCU0oZk5mZmZVOsR9+uAdFTILMzMzMoPBFt5d1LQJ2AY4FrntrCzMz\nM7PCFToasl+X9x3AC8C/Aj1eQWRmZmbWX4Uuuj282IGYmZmZ5VPsNSxmZmZmRVdQwiJplKSfSvqb\npA2SNua+ih2kmZmZDW6FrmGZBbwH+E/gWaB67u9vZmZmVafQhOXDwKERsbiYwZiZmZl1p9A1LE+T\nLmU2MzMzK7lCE5YvABdL2rV4oZiZmZl1r9ApoZ+THji4XNI64I3cjRGx45YGZmZmZtap0ITlC0WN\nop8kzQC+RHrQ4cPA5yLiwT60O4T0rKM/RER9SYM0MzOzoin0xnFlu/2+pJOBS4F/AR4AmoAWSXtF\nxKoe2tWRHhuwgPSUZzMzM6sSBT+oUNJQ4Hhg76zoT8C8iCj1fViagKsjYnYWx1mkZxh9ip6fEn0V\n8DPSYwSOK3GMZmZmVkSF3jhuLPAoMBv4WPaaA/xJ0h7FC+8tnzsMaAAWdpZFRJBGTcb30O4MYDfg\nglLFZmZmZqVT6FVC3wOWA2Mioj5bD/IeYEW2rVRGAkOBlV3KV5LWs7yFpD2BbwKnRERHCWMzMzOz\nEil0SmgicHBErOksiIjVkr4G3FOUyIpA0hDSNND5EbG8s7iv7Zuamqirq9usrLGxkcbGxuIFaWZm\nVqWam5tpbm7erKy9vb0kn6U0o9LPRtIa4KMRcW+X8kOA35TqsuZsSmgdcGJEzMspnwXURcQJXerX\nAS8CG9iUqAzJvt8AHBkRd3bzOfVAa2trK/X1vpjIzMysr9ra2mhoaABoiIi2Yu230Cmhm4AfSjpI\nmxxMWtg6r5e2BYuIN4BWYFJnmSRl7+/tpsla4APAvsC47HUVsCT7/v5SxWpmZmbFU+iU0LmkS4QX\nsemmccOAG4HPFyGunlwGzJLUyqbLmoeTHsiIpIuAd0XE6dmC3D/nNpb0PLA+Ih4tcZxmZmZWJIXe\nh+Ul4LjsaqF9suI/R8TjRYss/2fPlTQSuJB0P5XFwJSIeCGrMhoYU+o4zMzMbOBsyX1YziSNbuyZ\nFS2T9N2IuKYokfUgImYCM/NsO6OXthfgy5vNzMyqSkEJi6QLgS8C3ydNC0G6D8rlkt4TEf9RpPjM\nzMzMCh5hORv4TETkXss0T9IjpCTGCYuZmZkVTaFXCQ0Dft9NeStbMM1kZmZm1p1CE5afkkZZuvoX\n0o3azMzMzIqmz6Mhki7LeRvApyUdCdyXlR1Euj3/7OKFZ2ZmZta/6Zv9urxvzb52PuxwVfb6xy0N\nyszMzCxXnxOWiDi8lIGYmZmZ5VPoGhYzMzOzAeOExczMzCqeExYzMzOreE5YzMzMrOJVZcIiaYak\nFZJek3SfpAN6qHuCpPmSnpfULune7HJsMzMzqxJVl7BIOhm4FDifdKn1w0BL9gTn7kwA5gNHA/XA\nHcBvJI0bgHDNzMysCKouYSE9IfrqiJgdEUuAs4B1wKe6qxwRTRFxSUS0RsTyiPg6sAz4p4EL2czM\nzLZEVSUskoYBDcDCzrKICGAB6WnRfdmHgHcAa0oRo5mZmRVfVSUswEhgKLCyS/lKYHQf9/FlYAQw\nt4hxmZmZWQkNqicrS5oGnAdMjYhVvdVvamqirq5us7LGxkYaGxtLFKGZmVn1aG5uprm5ebOy9vb2\nknyW0oxKdcimhNYBJ0bEvJzyWUBdRJzQQ9tPANcAJ0XEbb18Tj3Q2traSn19fVFiNzMzGwza2tpo\naGgAaIiItmLtt6qmhCLiDdJDFyd1lmVrUiYB9+ZrJ6kR+DHwid6SFTMzM6s81TgldBkwS1Ir8ADp\nqqHhwCwASRcB74qI07P307Jt5wIPShqV7ee1iFg7sKGbmZlZIaouYYmIudk9Vy4ERgGLgSkR8UJW\nZTQwJqfJZ0gLda/MXp2uI8+l0GZmZlZZqi5hAYiImcDMPNvO6PL+8AEJyszMzEqmqtawmJmZ2eDk\nhMXMzMwqnhMWMzMzq3hOWMzMzKziVeWiWyuflpYW7r//fsaPH88RRxxR7nDMzGyQcMJifbJ8+XIO\nOugQVq/e9BinnXYaxYMPLmK33XZ7S30nNmZmVkxOWKxPUrKyHpgDTADuZvXqGRxwwHhWrXruzXr9\nTWzMzMz6wmtYrFctLS1ZAvLvwI7AeuAU4AesXr2S22+//c26++9/EKtXv7RZ+9WrX6K+/sABjNjM\nzGqNExbr1a233goI+DJwDLAXcCwwDoBFixYBKbF56aUXSU9KmAM8lX0dzksvrdkssTEzM+sPJyzW\no+XLl3PFFVcCuU/13he4B/goAOPHjwdgzpw5QAfwfdIIzJjs6/eAjmy7mZlZ/zlhsR7V1x8AjGDT\niMl3gMeB7bL34qSTTmbFihWkB2dDWuOyFLgVWAZMHPC4zcystlRlwiJphqQVkl6TdJ+kA3qpf5ik\nVknrJS2VdPpAxVrNWlpaWLv2RdIzI48GjiBNC70CPE2aJhrC2rWvcsAB4znllFOyllOB97Fp+mgq\nAKeeeurAHoCZmdWMqktYJJ0MXAqcD+wHPAy0ZE9w7q7+rsBNwELSoosrgGsk+VrbXtx///3Zd38E\n3gk8lrN1CGmaKIA3WL16JUOGDCFdeLaCzdewrAC28uXNZmZWsKpLWIAm4OqImB0RS4CzgHXAp/LU\nPxt4IiK+EhGPRcSVwC+y/Vgev/3tbzn//PNJp8i3gGFdagwD9gHekX0/hGOPPQ7YQBqRyV3D8gNg\ngxfdmplZwaoqYZE0DGggjZYAEBEBLADG52l2cLY9V0sP9Q2YPPko0mjJcNLUz1uv/IElpEud/w50\n8MYbnQtzJ3TZW1rD0nk1kZmZWX9VVcICjASGAiu7lK8ERudpMzpP/e0kbVPc8GrDN77xDSLeII2W\nHEJPV/7A8zktp2Vf7+6yx7uATVcTmZmZ9ZfvdNuDpqYm6urqNitrbGyksbGxTBENjIULF+a868i+\ndj9qkpYQdTodmAXMIK1tmUhKVj7LTjuN8hoWM7Ma09zcTHNz82Zl7e3tJfmsaktYVgEbgVFdykcB\nz721OmTl3dVfGxGv9/Rhl19+OfX19YXEWdUmTZrEHXfckb17b/b1btLISqe7sq+/I61hOYI0XdTB\ndtsNYe3a6W/W7Lw1v5mZ1Zbu/ohva2ujoaGh6J9VVVNCkeYpWoFJnWVKN/+YBNybp9mi3PqZI7Ny\n68bXv/510nKhrYAbgK1JoyZzSJczzwE+Szp9XictK5pK50hKe/sa5s+fzwUXXMD8+fNZteo5P0fI\nzMy2SLWNsABcBsyS1Ao8QLraZzhpLgJJFwHviojOe61cBcyQ9C3gJ6Tk5STSTUIsjzvvXMDEiYcB\nL5OmhTYC03NqDGXTdNF9wH2bjaQcccQRngIyM7OiqbqEJSLmZvdcuZA0tbMYmBIRL2RVRpNWhnbW\nf1LSscDlwLnAM8CZEdH1yiHLMWHCBCI6uPjiiznvvPPYsGHDZttHj34ns2fPBtLVP+PHj3eCYmZm\nJaN0VbDlklQPtLa2tg7KNSxmZmaFylnD0hARbcXab1WtYTEzM7PByQmLmZmZVTwnLGZmZlbxnLCY\nmZlZxXPCYmZmZhXPCYuZmZlVPCcsZmZmVvGcsJiZmVnFc8JiZmZmFc8Ji5mZmVU8JyxmZmZW8aoq\nYZG0g6SfSWqX9KKkaySN6KH+VpK+JekRSa9I+quk6yTtMpBxV7vm5uZyh1AR3A+buC8S90PiftjE\nfVE6VZWwANcDewOTgGOBCcDVPdQfDuwLXADsB5wAvA+4sbRh1hb/B0zcD5u4LxL3Q+J+2MR9UTpb\nlTuAvpL0fmAK6emPD2VlnwNulvSliHiua5uIWJu1yd3PZ4H7Jb07Ip4ZgNDNzMxsC1XTCMt44MXO\nZCWzAAjgoH7sZ/uszUtFjM3MzMxKqJoSltHA87kFEbERWJNt65WkbYCLgesj4pWiR2hmZmYlUfYp\nIUkXAV/toUqQ1q1s6edsBdyQ7e+cXqpvC/Doo49u6cfWhPb2dtra2sodRtm5HzZxXyTuh8T9sIn7\nYrPfndsWc7+KiGLur/8BSDsBO/VS7QlgOnBJRLxZV9JQYD1wUkTkXUibk6zsCnwkIl7sJaZpwM/6\ndABmZmbWnVMi4vpi7azsIywRsRpY3Vs9SYuA7SXtl7OOZRIg4P4e2nUmK7sDh/eWrGRagFOAJ0kJ\nkZmZmfXNtqQBgpZi7rTsIyz9IekWYGfgbGBr4CfAAxExPafOEuCrEXFjlqz8knRp80fZfA3Mmoh4\nY8CCNzMzs4KVfYSln6YBPyBdHdQB/AL4fJc6ewJ12ff/QEpUABZnX0Vax3I4cHcpgzUzM7PiqKoR\nFjMzMxucqumyZjMzMxuknLCYmZlZxXPCkunvgxWzNtdK6ujyumWgYi4GSTMkrZD0mqT7JB3QS/3D\nJLVKWi9pqaTTByrWUutPX0ia2M2//UZJOw9kzMUm6VBJ87IHhXZImtqHNjV3TvS3H2r4fPg3SQ9I\nWitppaT/lrRXH9rV4jnR776oxfNC0lmSHs5+V7ZLulfSUb20Kcr54IRlk/4+WLHTrcAo0t12RwON\npQqw2CSdDFwKnE96OOTDQIukkXnq7wrcBCwExgFXANdIOmIg4i2l/vZFJkiLvDv/7XeJiOd7qF8N\nRpAWqJ9DOr4e1fA50a9+yNTi+XAo8H3S408mA8OA+ZLelq9BDZ8T/e6LTK2dF0+TbvZaDzQAvwVu\nlNTtDV6Lej5ExKB/Ae8nXXW0X07ZFGADMLqHdtcCvyp3/Ftw3PcBV+S8F/AM8JU89b8FPNKlrBm4\npdzHUoa+mAhsBLYrd+wl7JMOYGovdWr2nOhnP9T8+ZAd58isPz48mM+JfvTFYDkvVgNnlPp88AhL\nsiUPVjwsGx5cImmmpB1LFmURSRpGyo4XdpZFOpMWkPqjOwdn23O19FC/KhTYF5CSmsWS/iZpvqQP\nlTbSilST50SBBsP50Pnw2DU91Bks50Rf+gJq+LyQNETSJ4DhwKI81Yp2PjhhSQp9sOKtwGnAR4Cv\nkLLpWySpRHEW00hgKLCyS/lK8h/z6Dz1t1N6sGS1KqQvngX+N3Ai8DHSMOmdkvYtVZAVqlbPif6q\n+fMh+7n2XeB3EfHnHqrW/DnRj76oyfNC0gckvQy8DswEToiIJXmqF+18qLYbx/WLSvxgxYiYm/P2\nT5L+ACwHDgPuKHS/VvkiYimwNKfoPkl7AE1A1S8wtP4ZJOfDTGAf4JByB1IB+tQXNXxeLCGtR6kD\nTgJmS5rQQ9JSFDWdsACXkNaZ9OQJ4DnSLf/fpPRgxR2zbX0SESskrQLGUvkJyyrS3OqoLuWjyH/M\nz+WpvzYiXi9ueAOqkL7ozgMMvh/mtXpOFEPNnA+SfgAcAxwaEc/2Ur2mz4l+9kV3qv68iIgNpN+d\nAA9JOpB01/mzu6letPOhpqeEImJ1RCzt5bWBNPe2vaT9cpr3+mDFriS9m/Tk6UJO4gEV6TlKraTj\nBN4c5pwE3Jun2aLc+pkjyT93WRUK7Ivu7EsV/NsXWU2eE0VSE+dD9gv6ONLDY5/qQ5OaPScK6Ivu\n1MR50cUQIN/0TvHOh3KvLq6UF3AL8HvgAFL2+xjw0y51lgDHZd+PAL5NWpT73uwf5PfAo8Cwch9P\nH4/548A60jqc95Mu414NvDPbfhFwXU79XYGXSau+30e65PPvwORyH0sZ+uLzwFRgD+AfSfPZbwCH\nlftYtrAfRpCGevclXQHxhez9mMF0ThTQD7V6PswEXiRd0jsq57VtTp1vDpJzopC+qLnzIjvGQ7Pf\nex/I/i9sAD6SbS/Zz4iyH3ylvEgrvucA7dlJ+SNgeJc6G4HTsu+3BW4jDXetJw2P/VfnL7hqeWUn\nz5PAa6SMd/+cbdcCv+1SfwJpNOI1YBkwvdzHUI6+AL6cHf+rwAukK4wmlPsYitAHE7Nf0Bu7vH4y\nmM6J/vZDDZ8P3fXBmz8HB9k50e++qMXzArgm+333Wvb7bz5ZslLq88EPPzQzM7OKV9NrWMzMzKw2\nOGExMzOziueExczMzCqeExYzMzOreE5YzMzMrOI5YTEzM7OK54TFzMzMKp4TFjMzs0FG0qGS5kn6\nq6QOSVP72f78rN3G7Gvn6+VSxeyExczMbPAZASwm3eG7kDvIfgcYDeySfR0N/BmYW6wAu3LCYmYV\nr5C/AM0sv4i4LSL+IyJuJD3odzOStpZ0iaRnJL0iaZGkiTnt10XE850vUuKyD/DjUsXshMXMzMy6\nupL0cN+PAx8EbgBulbRHnvqfBh6LiP484b5fnLCYWc2TNKzcMZhVC0ljgE8C/xwR90bEioi4DLgH\nOKOb+tsA00gPRiwZJyxmNiAknSTpEUnrJK2SNF/S2yTtn33/gqSXJN0pab9e9nWxpMckvSppuaQL\nJQ3N2X6+pIcknSnpCeA1SdOzzx3WZV+/lnRdiQ7brBp9EBgKLJX0cueL9NTl7kZYPga8HZhdyqC2\nKuXOzcwAJI0Grge+BPwaeAdwKGnu/B3ALGAG6Y+ofwVukTQ2Il7Ns8u1wGnAs6Qfrj/Kyi7JqTOW\n9IP0BGAj8DhwBTAV+GUW1zuBY4DJxTlSs5rwdmADUA90dNn2Sjf1zwRuiogXShmUExYzGwi7kP5i\n+++IeDor+1P29Y7cipLOAk4GJgK3dLeziPhmztunJF2atclNWIYB0yNiTc6+m0lD2r/MiqYDf4mI\nuws5KLMa9RDp/+uoiLinp4qSdgUOBz5a6qCcsJjZQHgYWAj8UVILMB/4RUS8JGln4BukBGVn0g/K\ntwHvybczSScDnyMNT7+d9LOsvUu1v+QmK5kfAQ9I2iUingVOB67d0oMzqzaSRpBGITuvENpd0jhg\nTUQsk3S8Qt8EAAAB3ElEQVQ9MFvSl0gJzM7AR4CHI+LWnF2dCfwNuK3UMXsNi5mVXER0RMSRwFGk\nkZXPAUuyv85mA/8rKxsPjAPWAFt3ty9J44E5wE3AscC+pISna/23TCdFxGLgEeA0SfWkyzC9fsUG\no/1JiUgr6T4slwJtwAXZ9k+S/m9eAiwBfpW1eapzB5JElvRHRCH3cukXj7CY2YCJiEXAIkn/CfyF\ntL7kQ8DZEdECb16hMLKH3YwHnoyIizsLssSnr64BvgC8G1gQEX/tzzGY1YKIuIseBi0iYiMpebmg\nhzpBDyOhxeaExcxKTtKBwCTSVNDzwMGkpOTPwFJguqRWoA74NrCuh90tA96TTQs9SJo7P74f4VxP\n+qvx06Q1LGZWBTwlZGYDYS3pksibgceAC4EvZqMqnwZ2IA1NX0e6kuf5Lu3fHG6OiN8AlwPfJw1p\nH5ztr08iYi1p0e0rwI2FHY6ZDTQNwLSTmVlFkbQA+ENENJU7FjPrG08JmdmgIWl70iWYE4GzyxyO\nmfWDExYzG0weArYHvhIRy8odjJn1naeEzMzMrOJ50a2ZmZlVPCcsZmZmVvGcsJiZmVnFc8JiZmZm\nFc8Ji5mZmVU8JyxmZmZW8ZywmJmZWcVzwmJmZmYV7/8DO8RUnGktJZoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d157f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load poi_id_submit2.py\n",
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "# read the all data\n",
    "## Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# data overview\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.T\n",
    "number_of_non_poi, number_of_poi = df['poi'].value_counts()\n",
    "print \"number of columns:\", len(df.columns)\n",
    "print \"name of the columns:\\n\", df.columns\n",
    "print \"number of non pois / pois:\", number_of_non_poi, '/', number_of_poi\n",
    "\n",
    "all_features_list = df.columns\n",
    "nan_index = df.columns\n",
    "nan_columns = ['non_nan', 'nan', 'nan_poi','nan_non_poi', 'poi_nan_ratio', 'non_poi_nan_ratio']\n",
    "df_nan = pd.DataFrame(index=nan_index, columns=nan_columns)\n",
    "df_nan = df_nan.fillna(0)\n",
    "\n",
    "for i in all_features_list:\n",
    "    for j in df.index:\n",
    "        if df[i][j] == 'NaN':\n",
    "            df_nan['nan'][i]+=1\n",
    "            if df['poi'][j] == True:\n",
    "                df_nan['nan_poi'][i]+=1\n",
    "            else:\n",
    "                df_nan['nan_non_poi'][i]+=1\n",
    "        else:\n",
    "            df_nan['non_nan'][i]+=1\n",
    "\n",
    "# poi_nan_ratio column\n",
    "for i in all_features_list:\n",
    "    if df_nan['nan_poi'][i] == 0:\n",
    "        df_nan['poi_nan_ratio'][i] == 'NaN'\n",
    "    else:\n",
    "        df_nan['poi_nan_ratio'][i] = round((df_nan['nan_poi'][i])*100.0/number_of_poi, 2)\n",
    "\n",
    "# non_poi_nan_ratio column\n",
    "for i in all_features_list:\n",
    "    if df_nan['nan_non_poi'][i] == 0:\n",
    "            df_nan['non_poi_nan_ratio'][i] == 'NaN'\n",
    "    else:\n",
    "        df_nan['non_poi_nan_ratio'][i] = round((df_nan['nan_non_poi'][i])*100.0/number_of_non_poi, 2)\n",
    "\n",
    "df_nan_1 = df_nan[['non_nan', 'nan']].sort_values(by=['nan'], ascending = True)\n",
    "df_nan_2 = df_nan[['poi_nan_ratio', 'non_poi_nan_ratio']].sort_values(by=['poi_nan_ratio'], ascending = True)\n",
    "\n",
    "\n",
    "print 'Number of missing values in data'       \n",
    "print df_nan_1['nan']\n",
    "\n",
    "print 'Ratio of missing values in POI / non-POIs'     \n",
    "print df_nan_2\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(6,2))\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# plot the salary and the bonus data\n",
    "features_outliers = [\"salary\", \"bonus\"]\n",
    "data_outliers = featureFormat(data_dict, features_outliers)\n",
    "\n",
    "for point in data_outliers:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter(salary,bonus)\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\");\n",
    "\n",
    "# find the key of outlier\n",
    "for key, value in data_dict.items():\n",
    "    if value['bonus'] == data_outliers.max():\n",
    "        print 'max:', key, data_outliers.max()\n",
    "\n",
    "# remove the outlier 'TOTAL'\n",
    "data_dict.pop('TOTAL', 0 )\n",
    "\n",
    "# second plot\n",
    "data_outliers = featureFormat(data_dict, features_outliers)\n",
    "for point in data_outliers:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter(salary,bonus)\n",
    "    \n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\");\n",
    "\n",
    "outlier_bonus =5000000\n",
    "outlier_salary = 500000\n",
    "\n",
    "both_outlier = []\n",
    "for key, value in data_dict.items():\n",
    "    if value['bonus'] == 'NaN':\n",
    "        pass\n",
    "    elif value['bonus'] > outlier_bonus:\n",
    "        if value['salary'] > outlier_salary:\n",
    "            both_outlier.append(key)\n",
    "    else:\n",
    "        pass\n",
    "print 'people whose both bonus and salary are away from other data points'   \n",
    "print both_outlier\n",
    "\n",
    "bonus_outlier = []\n",
    "for key, value in data_dict.items():\n",
    "    if value['bonus'] == 'NaN':\n",
    "        pass\n",
    "    elif value['bonus'] > outlier_bonus:\n",
    "        if value['salary'] > outlier_salary:\n",
    "            pass\n",
    "        else:\n",
    "            bonus_outlier.append(key)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print 'people whose only bonus is away from other data points'       \n",
    "print bonus_outlier\n",
    "\n",
    "salary_outlier = []\n",
    "for key, value in data_dict.items():\n",
    "    if value['salary'] == 'NaN':\n",
    "        pass\n",
    "    elif value['salary'] > outlier_salary:\n",
    "        if value['bonus'] > outlier_bonus:\n",
    "            pass\n",
    "        else:\n",
    "            salary_outlier.append(key)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print 'people whose only bonus is away from other data points'       \n",
    "print salary_outlier\n",
    "\n",
    "# find poi/non-poi of outlier\n",
    "outlier_list = both_outlier + bonus_outlier + salary_outlier\n",
    "\n",
    "poi_outlier = []\n",
    "non_poi_outlier = []\n",
    "for key in outlier_list:\n",
    "    poi_value = df.get_value(index=key, col='poi')\n",
    "    if poi_value == True:\n",
    "        poi_outlier.append(key)\n",
    "        print '{:>20}: {}'.format(key, poi_value)\n",
    "    else:\n",
    "        non_poi_outlier.append(key)  \n",
    "        print '{:>20}: {}'.format(key, poi_value)\n",
    "\n",
    "# remove the non-poi outlier\n",
    "for key in non_poi_outlier:\n",
    "    data_dict.pop(key, 0 )\n",
    "\n",
    "# total number of poi nd non-poi after outlier removal    \n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.T\n",
    "number_of_non_poi, number_of_poi = df['poi'].value_counts()\n",
    "print \"number of non-pois / pois:\", number_of_non_poi, '/', number_of_poi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# all features initially available\n",
    "features_list = list(all_features_list)\n",
    "# remove email address from all features\n",
    "features_list.remove('email_address')\n",
    "\n",
    "### Extract features and labels from dataset again with the new features_list\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# split data into train and test data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# loop over several models to find the best one [1][5]\n",
    "\n",
    "def clf_df(features_train, labels_train, features_test, labels_test):\n",
    "    classifiers = [\n",
    "        GaussianNB(),\n",
    "        svm.SVC(),\n",
    "        DecisionTreeClassifier()\n",
    "    ]\n",
    "\n",
    "    clf_col = ['clf', 'score', 'precision', 'recall']\n",
    "    clf_df = pd.DataFrame(index=[], columns=clf_col)\n",
    "\n",
    "    average_score = 0\n",
    "    for clf in classifiers:\n",
    "        steps = [('clf', clf)]\n",
    "        pipeline = Pipeline(steps)\n",
    "        clf.fit(features_train, labels_train)\n",
    "        pred = clf.predict(features_test)\n",
    "        score = clf.score(features_test, labels_test)\n",
    "        precision = precision_score(labels_test, pred, average='weighted')\n",
    "        recall = recall_score(labels_test, pred, average='weighted') \n",
    "\n",
    "        series = pd.Series([clf, score, precision, recall], index=clf_df.columns)\n",
    "        clf_df = clf_df.append(series, ignore_index = True)\n",
    "\n",
    "    print 'accuracy, precision and recall scores of each classifier'\n",
    "    pd.set_option('display.width', 100)\n",
    "    print clf_df\n",
    "\n",
    "clf_df(features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "# from the clf_data result, choose the best algorithm to predict poi / non-poi\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# features scaling is not needed with DecisionTreeClassifier() in general [3][4]\n",
    "\n",
    "# feature importances on decision tree classifier\n",
    "clf.fit(features, labels)\n",
    "FeatureImportances = clf.feature_importances_\n",
    "FeatureImportances = sorted(FeatureImportances, reverse=True)\n",
    "features_list_rm_poi = list(features_list)\n",
    "features_list_rm_poi.pop(0)\n",
    "\n",
    "print 'feature importances by order'\n",
    "for i, feat in enumerate(features_list_rm_poi):\n",
    "    print feat, ':', FeatureImportances[i]\n",
    "    \n",
    "\n",
    "    \n",
    "### Task 3: Create new feature(s)\n",
    "from fractions import Fraction\n",
    "\n",
    "### new feature 1: portion of from/to poi messages within all from/to messages\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "   \"\"\"\n",
    "\n",
    "    fraction = 0.\n",
    "    if 'email_address' == 'NaN':\n",
    "        return 0\n",
    "    else:\n",
    "        if all_messages == 0 or all_messages == 'NaN':\n",
    "            fraction = 0\n",
    "        if poi_messages == 'NaN':\n",
    "            fraction = 0\n",
    "        else:\n",
    "            fraction = Fraction(poi_messages, all_messages)\n",
    "    return fraction\n",
    "\n",
    "\n",
    "for name in my_dataset:\n",
    "\n",
    "    data_point = my_dataset[name]\n",
    "\n",
    "    from_poi_to_this_person = data_point['from_poi_to_this_person']\n",
    "    to_messages = data_point['to_messages']\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    data_point['fraction_from_poi'] = fraction_from_poi\n",
    "\n",
    "    from_this_person_to_poi = data_point['from_this_person_to_poi']\n",
    "    from_messages = data_point['from_messages']\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    data_point['fraction_to_poi'] = fraction_to_poi\n",
    "\n",
    "# add new features to features_list\n",
    "features_list.extend(['fraction_from_poi', 'fraction_to_poi'])\n",
    "\n",
    "# try the classifiers with new feature list\n",
    "### Extract features and labels from dataset again with the new features_list\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# split data into train and test data\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "clf_df(features_train, labels_train, features_test, labels_test)\n",
    "\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "# select features for Decision Tree Classifier using SelectKBest, connecting with Pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# features_list and data for now\n",
    "# features_list\n",
    "features_list = list(all_features_list)\n",
    "features_list.remove('email_address')\n",
    "features_list.extend(['fraction_from_poi', 'fraction_to_poi'])\n",
    "\n",
    "# data \n",
    "### Extract features and labels from dataset again with the new features_list\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# split data into train and test data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "\n",
    "\n",
    "n_features = np.arange(1, len(features_list))\n",
    "kbest = SelectKBest(f_classif)\n",
    "pipeline = Pipeline([('kbest', kbest), ('DT', DecisionTreeClassifier())])\n",
    "\n",
    "parameters = {\n",
    "    'kbest__k':n_features\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=10)\n",
    "\n",
    "grid_search.fit(features_train, labels_train)\n",
    "grid_scores = pd.DataFrame(grid_search.grid_scores_)\n",
    "\n",
    "print 'grid search score'\n",
    "pd.set_option('display.width', 100)\n",
    "print grid_scores\n",
    "\n",
    "print '\\nbest parameters set'\n",
    "best_k = grid_search.best_params_\n",
    "print best_k\n",
    "\n",
    "clf = grid_search\n",
    "pred_kbest = clf.predict(features_test)\n",
    "score = clf.score(features_test, labels_test)\n",
    "precision = precision_score(labels_test, pred_kbest, average='weighted')\n",
    "recall = recall_score(labels_test, pred_kbest, average='weighted')\n",
    "\n",
    "print '\\nevaluation of classifier when k is best_k'\n",
    "print 'score:', score, 'precision:', precision, 'recall:', recall\n",
    "\n",
    "# when k=best_k for SelectKBest\n",
    "kbest = SelectKBest(f_classif, k=19)\n",
    "\n",
    "# remove labels('poi') from the features_list\n",
    "features_list_rm_poi = features_list\n",
    "features_list_rm_poi.remove('poi')\n",
    "\n",
    "features_df = pd.DataFrame(features, columns=features_list_rm_poi)\n",
    "labels_df = pd.DataFrame(labels)\n",
    "\n",
    "kbest.fit(features, labels)\n",
    "features_new = kbest.transform(features)\n",
    "kbest_features = features_df.columns[kbest.get_support(indices=True)].tolist()\n",
    "kbest_scores = kbest.scores_\n",
    "\n",
    "kbest_features = []\n",
    "for i in np.argsort(kbest_scores):\n",
    "    kbest_features.append(features_list_rm_poi[i])\n",
    "\n",
    "kbest_scores = list(kbest_scores)    \n",
    "kbest_scores = sorted(kbest_scores, reverse=True)\n",
    "\n",
    "# print 10 best features and its scores\n",
    "for i in range(19):\n",
    "    print '{:>25}: {}'.format(kbest_features[i], kbest_scores[i])\n",
    "\n",
    "# rewrite the features_list with the best_k best features \n",
    "features_list = ['poi'] + kbest_features[:19]\n",
    "\n",
    "\n",
    "# data with new feature list\n",
    "### Extract features and labels from dataset again with the new features_list\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# split data into train and test data\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "# [8]\n",
    "\n",
    "# select features using SelectKBest and GridSearch CV, connecting them with pipeline [3][6][7]\n",
    "\n",
    "n_features = np.arange(1, len(features_list))\n",
    "kbest = SelectKBest(f_classif, k=17)\n",
    "pipeline = Pipeline([('kbest', kbest), ('DT', DecisionTreeClassifier())])\n",
    "\n",
    "parameters = {\n",
    "    'DT__criterion':['gini', 'entropy'], \n",
    "    'DT__min_samples_split':[2, 4, 6, 8, 10, 20],\n",
    "    'DT__max_depth':[None, 5, 10, 15, 20],\n",
    "    'DT__min_samples_leaf':[2, 4, 6, 8, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=10)\n",
    "\n",
    "grid_search.fit(features_train, labels_train)\n",
    "grid_search.grid_scores_\n",
    "\n",
    "print 'best parameters set'\n",
    "print grid_search.best_params_\n",
    "\n",
    "pred_best_model = grid_search.predict(features_test)\n",
    "score = grid_search.score(features_test, labels_test)\n",
    "precision = precision_score(labels_test, pred_best_model, average='weighted')\n",
    "recall = recall_score(labels_test, pred_best_model, average='weighted')\n",
    "print score, precision, recall\n",
    "\n",
    "clf = grid_search\n",
    "print clf\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', min_samples_split=2, max_depth=None, min_samples_leaf=8)\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
